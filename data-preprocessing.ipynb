{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIDENTIEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-blob in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (12.18.3)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from azure-storage-blob) (1.29.5)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from azure-storage-blob) (41.0.5)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from azure-storage-blob) (4.8.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from azure-storage-blob) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyyaml in /Users/elodieroyant/anaconda3/envs/hackatech-simple/lib/python3.11/site-packages (6.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-storage-blob\n",
    "%pip install pyyaml\n",
    "%pip install pydub\n",
    "%pip install praat-parselmouth\n",
    "%pip install azure-cognitiveservices-speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your local path, for example:\n",
    "# /Users/elodieroyant/Documents/StudioVRAI\n",
    "LOCAL_PATH = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import env variables (you should retrieve env in secure notes and save as a .env.yaml file at root file)\n",
    "import yaml\n",
    "with open('.env.yaml', 'r') as stream:\n",
    "    data = yaml.safe_load(stream).split(':')\n",
    "env = { data[0]: data[1] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def format_labels(json_data):\n",
    "    id = json_data['id']\n",
    "    file_name = json_data['task']['data']['video_url'].split('/')[-1]\n",
    "    results_with_none = list(map(lambda x: x['value'] if x['type'] == 'rating' else None, json_data['result']))\n",
    "    results = list(filter(lambda x: x is not None, results_with_none))\n",
    "    rows = list(map(lambda x: { \"id\": id, \"file_name\": file_name, \"rating\": x[\"rating\"], \"start\": x[\"start\"],\"end\": x[\"end\"], \"duration\": x[\"end\"] - x[\"start\"] }, results))\n",
    "    return rows\n",
    "\n",
    "headers = [\"id\", \"file_name\", \"rating\", \"start\", \"end\",  \"duration\"]\n",
    "\n",
    "with open(LOCAL_PATH + f\"/formated-labels/tchatche.csv\", \"w\", newline='') as outfile:\n",
    "    w = csv.DictWriter(outfile, headers)\n",
    "    w.writeheader()\n",
    "    for i in range(8, 131):\n",
    "        # read dw json files and write them to a csv\n",
    "        with open(LOCAL_PATH + f\"/raw-labels/{i}.json\", \"r\") as file:\n",
    "            json_data = json.load(file)\n",
    "            rows = format_labels(json_data)\n",
    "            for row in rows:\n",
    "                w.writerow(row)\n",
    "\n",
    "with open(LOCAL_PATH + f\"/formated-labels/tchatche.csv\", newline='') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    files = []\n",
    "    for row in reader:\n",
    "        if (row['file_name'] not in files):\n",
    "            files.append(row['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a file from blob storage on local path\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Create a BlobServiceClient object using your connection string\n",
    "blob_service_client = BlobServiceClient.from_connection_string(env['AZURE_BLOB_STORAGE_CONNECTION_STRING'])\n",
    "# Specify the container name and blob name\n",
    "container_name = \"raw\"\n",
    "\n",
    "for blob_name in files:\n",
    "    # Get a reference to the blob\n",
    "    blob_client = blob_service_client.get_blob_client(container_name, blob_name)\n",
    "\n",
    "    # Download the blob content\n",
    "    downloaded_content = blob_client.download_blob().readall()\n",
    "\n",
    "    # Save the downloaded content to a file\n",
    "    with open(LOCAL_PATH + f\"/raw-data/{blob_name}\", \"wb\") as file:\n",
    "        file.write(downloaded_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract audio from video\n",
    "import subprocess\n",
    "\n",
    "with open(LOCAL_PATH + f\"/formated-labels/tchatche.csv\", newline='') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    files = []\n",
    "    for row in reader:\n",
    "        file_name = row['file_name'].replace('.mp4', '')\n",
    "        if (file_name not in files):\n",
    "            files.append(file_name)\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    command = f\"ffmpeg -i {LOCAL_PATH}/raw-data/{file}.mp4 -ab 160k -ac 2 -ar 44100 -vn {LOCAL_PATH}/raw-audio/{file}.wav\"\n",
    "    subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file segmentation on labels\n",
    "from pydub import AudioSegment\n",
    "\n",
    "with open(LOCAL_PATH + f\"/formated-labels/tchatche.csv\", newline='') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    files = []\n",
    "    for row in reader:\n",
    "        file_name = row['file_name'].replace('.mp4', '')\n",
    "        start = int(float(row['start']) * 1000)\n",
    "        end = int(float(row['end']) * 1000)\n",
    "        path = f'{LOCAL_PATH}/raw-audio/{file_name}.wav'\n",
    "        print(path)\n",
    "        newAudio = AudioSegment.from_wav(path)\n",
    "        newAudio = newAudio[start:end]\n",
    "        newAudio.export(f'{LOCAL_PATH}/split-audio/{file_name}_{start}_{end}.wav', format=\"wav\") #Exports to a wav file in the current path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new csv with correct file name\n",
    "from pydub import AudioSegment\n",
    "\n",
    "headers = [\"id\", \"file_name\", \"rating\", \"duration\"]\n",
    "\n",
    "def parse_tsp_in_s(tps_in_s):\n",
    "    return int(float(tps_in_s*1000))\n",
    "\n",
    "def format_labels(json_data):\n",
    "    file_name = json_data['task']['data']['video_url'].split('/')[-1].replace('.mp4', '')\n",
    "    results_with_none = list(map(lambda x: x['value'] if x['type'] == 'rating' else None, json_data['result']))\n",
    "    results = list(filter(lambda x: x is not None, results_with_none))\n",
    "    rows = list(map(lambda x: { \"file_name\": f'{file_name}_{parse_tsp_in_s(x[\"start\"])}_{parse_tsp_in_s(x[\"end\"])}.wav', \"rating\": x[\"rating\"], \"duration\": x[\"end\"] - x[\"start\"] }, results))\n",
    "    return rows\n",
    "\n",
    "headers = [\"file_name\", \"rating\", \"duration\"]\n",
    "\n",
    "with open(LOCAL_PATH + f\"/formated-labels/tchatche_split.csv\", \"w\", newline='') as outfile:\n",
    "    w = csv.DictWriter(outfile, headers)\n",
    "    w.writeheader()\n",
    "    for i in range(8, 131):\n",
    "        # read dw json files and write them to a csv\n",
    "        with open(LOCAL_PATH + f\"/raw-labels/{i}.json\", \"r\") as file:\n",
    "            json_data = json.load(file)\n",
    "            rows = format_labels(json_data)\n",
    "            for row in rows:\n",
    "                w.writerow(row)\n",
    "\n",
    "with open(LOCAL_PATH + f\"/formated-labels/tchatche_split.csv\", newline='') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    files = []\n",
    "    for row in reader:\n",
    "        if (row['file_name'] not in files):\n",
    "            files.append(row['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parselmouth\n",
    "import numpy as np\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=env['AZURE_SPEECH_KEY'], region=env['AZURE_SPEECH_REGION'], speech_recognition_language=\"fr-FR\")\n",
    "\n",
    "headers = [\"file_name\", \"rating\", \"duration\", \"transcript\", \"speed\",\\\n",
    "           \"intensity_avg\", \"intensity_std\", \"intensity_min\", \"intensity_max\", \\\n",
    "            \"pitch_avg\", \"pitch_std\", \"pitch_min\", \"pitch_max\",\n",
    "            \"mfcc0_avg\", \"mfcc0_std\", \"mfcc0_min\", \"mfcc0_max\",\n",
    "            \"mfcc1_avg\", \"mfcc1_std\", \"mfcc1_min\", \"mfcc1_max\",\n",
    "            \"mfcc2_avg\", \"mfcc2_std\", \"mfcc2_min\", \"mfcc2_max\"]\n",
    "\n",
    "new_rows = []\n",
    "with open(LOCAL_PATH + f\"/data-csv/tchatche_split.csv\", \"r\", newline='') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    for row in reader:\n",
    "        new_rows.append(row)\n",
    "\n",
    "row = new_rows[0]\n",
    "snd = parselmouth.Sound(LOCAL_PATH + f\"/raw-audio/{row['file_name']}\")\n",
    "# intensity = snd.to_intensity()\n",
    "# print(intensity.values)\n",
    "# print(intensity.values[0])\n",
    "# pitch = snd.to_pitch().selected_array['frequency']\n",
    "# print(pitch)\n",
    "# pitch = pitch[pitch != 0]\n",
    "# print(pitch)\n",
    "# mfcc_object = snd.to_mfcc(number_of_coefficients=12)\n",
    "# mfcc = mfcc_object.to_array()\n",
    "# print(mfcc_object)\n",
    "# print(mfcc.shape)\n",
    "# for i in range(0,12): \n",
    "#  print('avg', np.mean(mfcc[i]))\n",
    "#  print('std', np.std(mfcc[i]))\n",
    "#  print('min', np.min(mfcc[i]))\n",
    "#  print('max', np.max(mfcc[i]))\n",
    "# audio_config = speechsdk.audio.AudioConfig(filename=LOCAL_PATH + f\"/raw-audio/{row['file_name']}\")\n",
    "# speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "# result = speech_recognizer.recognize_once()\n",
    "# transcript = result.text\n",
    "# speed = len(transcript) * 60 / float(row['duration'])\n",
    "# print(speed)\n",
    "\n",
    "\n",
    "with open(LOCAL_PATH + f\"/data-csv/tchatche_split_x.csv\", \"w\", newline='') as csv_file:\n",
    "    w = csv.DictWriter(csv_file, headers)\n",
    "    w.writeheader()\n",
    "    for row in new_rows:\n",
    "        audio_config = speechsdk.audio.AudioConfig(filename=LOCAL_PATH + f\"/raw-audio/{row['file_name']}\")\n",
    "        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "        result = speech_recognizer.recognize_once()\n",
    "        row['transcript'] = result.text\n",
    "        row['speed'] = len(row['transcript'].split(\" \")) * 60 / float(row['duration'])\n",
    "        snd = parselmouth.Sound(LOCAL_PATH + f\"/raw-audio/{row['file_name']}\")\n",
    "        intensity = snd.to_intensity()\n",
    "        row['intensity_avg'] = np.mean(intensity.values[0])\n",
    "        row['intensity_std'] = np.std(intensity.values[0])\n",
    "        row['intensity_min'] = np.min(intensity.values[0])\n",
    "        row['intensity_max'] = np.max(intensity.values[0])\n",
    "        pitch = snd.to_pitch().selected_array['frequency']\n",
    "        pitch = pitch[pitch != 0]\n",
    "        row['pitch_avg'] = np.mean(pitch)\n",
    "        row['pitch_std'] = np.std(pitch)\n",
    "        row['pitch_min'] = np.min(pitch)\n",
    "        row['pitch_max'] = np.max(pitch)\n",
    "        mfcc_object = snd.to_mfcc(number_of_coefficients=12)\n",
    "        mfcc = mfcc_object.to_array()\n",
    "        for i in range(0,3): \n",
    "            row[f'mfcc{i}_avg'] = np.mean(mfcc[i])\n",
    "            row[f'mfcc{i}_std'] = np.std(mfcc[i])\n",
    "            row[f'mfcc{i}_min'] = np.min(mfcc[i])\n",
    "            row[f'mfcc{i}_max'] = np.max(mfcc[i])\n",
    "        w.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackatech-simple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
